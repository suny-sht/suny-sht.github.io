<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Aleksandar (Suny) Shtedritski</title>
  
  <meta name="author" content="Aleksandar (Suny) Shtedritski">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Aleksandar (Suny) Shtedritski</name>
              </p>
              <p>Hi, I am currently a third-year DPhil (PhD) student at <a href="https://www.robots.ox.ac.uk/~vgg/">Visual Geometry Group</a>, <a href="https://www.ox.ac.uk/">Oxford</a>, and am lucky to be advised by <a href="https://www.robots.ox.ac.uk/~vedaldi/">Prof. Andrea Vedaldi</a> and <a href="https://chrirupp.github.io/">Prof. Christian Rupprecht</a>. This summer currently interning at <a href="https://deepmind.google/">Google DeepMind</a> with <a href="https://www.robots.ox.ac.uk/~ow/">Olivia Wiles</a>.
               </p>
              <p> Before that, I graduated with an MEng in Engineering Science from the Univesity of Oxford. During my PhD I started a student research group within the <a href="https://www.oxai.org/">AI society</a> in Oxford, <a href="https://www.oxai.org/labs">OxAI Labs </a>, where we do research on fairness and bias. 
              </p>
              <p>
                My nickname is to be pronounced like "Sunny" &#9728;&#65039; despite missing an "n".
              </p>
              <p style="text-align:center">
                <a href="mailto:suny@robots.ox.ac.uk">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=cGnonsQAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/aleksandar-shtedritski-152a09134/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/suny-sht">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/suny.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/suny-circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in vision + language, un/self-supervised computer vision, 3D, and fairness.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collaps:eseparate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/hellofresh.png" alt="b3do" width="160" height="110" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2310.10632.pdf">
                <papertitle>HelloFresh: LLM Evaluations on Streams of Real-World Human Editorial Actions across X Community Notes and Wikipedia edits</papertitle>
              </a>
              <br>
              <a href="https://www.robots.ox.ac.uk/~frtim/">Tim Franzmeyer*</a>,
              <strong> Aleksandar Shtedritski*</strong>,
              <a href="https://samuelalbanie.com/">Samuel Albanie</a>,
              <a href="https://eng.ox.ac.uk/people/philip-torr/">Philip Torr</a>,
              <a href="https://www.robots.ox.ac.uk/~joao/">Joao F. Henriques</a>,
              <a href="https://www.jakobfoerster.com/">Jakob E. Foerster</a>,
              <br>
              <em>ACL</em>, 2024 &nbsp 
              <br>
              <a href="https://arxiv.org/abs/2406.03428">arXiv</a> /
              <a href="https://www.robots.ox.ac.uk/~vgg/research/HelloFresh/">website</a> /
              <a href="https://github.com/fratim/HelloFresh">code</a>
              <p>A living benchmark for LLMs that addresses issues like test data contamination and benchmark overfitting.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bioplanner.png" alt="b3do" width="160" height="140" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2310.10632.pdf">
                <papertitle>BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</papertitle>
              </a>
              <br>
              <a href="https://www.bdi.ox.ac.uk/Team/odhran-odonoghue">Odhran O'Donoghue</a>,
              <strong> Aleksandar Shtedritski</strong>,
              John Ginger,
              Ralph Abboud, 
              Ali Essa Ghareeb,
              Justin Booth,
              Samuel G Rodriques
              <br>
              <em>EMNLP</em>, 2023 &nbsp 
              <br>
              <a href="https://arxiv.org/abs/2310.10632">arXiv</a> /
              <!--<a href="data/visogender.bib">bibtex</a> /-->
              <a href="https://github.com/bioplanner/bioplanner">code</a>
              <p>A framework for evaluation of LLMs on long planning tasks, with an application in biology.</p>
            </td>
          </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/red-circle.png" alt="b3do" width="140" height="140" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2304.06712.pdf">
                <papertitle>What does CLIP know about a red circle? Visual prompt engineering for VLMs</papertitle>
              </a>
              <br>
              <strong> Aleksandar Shtedritski</strong>,
              <a href="https://chrirupp.github.io/">Christian Rupprecht</a>,
              <a href="https://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>

              <br>
              <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2304.06712">arXiv</a> /
              <!--<a href="data/red-circle.bib">bibtex</a> /-->
              <a href="https://github.com/suny-sht/clip-red-circle">code</a>
              <p>We discover an emergent ability of CLIP, where drawing a red circle focuses the global image description to the region inside the circle.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/corresp.png" alt="b3do" width="160" height="110" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Shtedritski_Learning_Universal_Semantic_Correspondences_with_No_Supervision_and_Automatic_Data_ICCVW_2023_paper.pdf">
                <papertitle>Learning Universal Semantic Correspondences with No Supervision and Automatic Data Curation</papertitle>
              </a>
              <br>
              <strong> Aleksandar Shtedritski</strong>,
              <a href="https://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>,
              <a href="https://chrirupp.github.io/">Christian Rupprecht</a>
              

              <br>
              <em>ICCV Workshop ion Representation Learning with Limited Data</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Shtedritski_Learning_Universal_Semantic_Correspondences_with_No_Supervision_and_Automatic_Data_ICCVW_2023_paper.pdf">paper</a> /
              <!--<a href="data/red-circle.bib">bibtex</a> /-->
              <p>We present a method for learning robust and generalizable semantic correspondences.</p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/visogender.png" alt="b3do" width="160" height="140" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2306.12424.pdf">
                <papertitle>VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution</papertitle>
              </a>
              <br>
              <a href="https://github.com/smhall97">Siobhan Mackenzie Hall</a>,
              <a href="https://github.com/abrantesfg">Fernanda Gon√ßalves Abrantes</a>,
              <a href="https://github.com/hanwenzhu">Hanwen Zhu</a>,
              <a href="https://github.com/grace-sodunke">Grace Sodunke</a>,
              <strong> Aleksandar Shtedritski</strong>,
              <a href="https://www.hannahrosekirk.com/">Hannah Rose Kirk</a>
              <br>
              <em>NeurIPS Datasets and Benchmarks</em>, 2023 &nbsp 
              <br>
              <a href="https://arxiv.org/abs/2306.12424">arXiv</a> /
              <!--<a href="data/visogender.bib">bibtex</a> /-->
              <a href="https://github.com/oxai/visogender">code</a>
              <p>A new dataset for benchmarking gender bias in vision-language models.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/balancing-the-picture.png" alt="b3do" width="160" height="120" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2305.15407.pdf">
                <papertitle>Balancing the Picture: Debiasing Vision-Language Datasets with Synthetic Contrast Sets</papertitle>
              </a>
              <br>
              <a href="https://brandonsmith.co.uk/">Brandon Smith</a>,
              <a href="https://github.com/mlfarinha">Miguel Farinha</a>,
              <a href="https://github.com/smhall97">Siobhan Mackenzie Hall</a>,
              <a href="https://www.hannahrosekirk.com/">Hannah Rose Kirk</a>,
              <strong> Aleksandar Shtedritski</strong>,
              <a href="https://maxbain.com/">Max Bain</a>
              <br>
              <em>NeurIPS Workshop SyntheticData4ML</em>, 2023 &nbsp 
              <br>
              <a href="https://arxiv.org/abs/2305.15407">arXiv</a> /
              <!--<a href="data/balancing.bib">bibtex</a> /-->
              <a href="https://github.com/oxai/debias-gensynth">code</a>
              <p>We demonstrate that the datasets used to evaluate the bias of VLMs are themselves biased. We propose to debias these datasets using synthetic contrast sets.</p>
            </td>
          </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/prompt-array.png" alt="b3do" width="160" height="80" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2203.11933.pdf">
                <papertitle>A prompt array keeps the bias away: Debiasing vision-language models with adversarial learning</papertitle>
              </a>
              <br>
              <a href="https://github.com/Drummersbrother">Hugo Bergh</a>,
              <a href="https://github.com/smhall97">Siobhan Mackenzie Hall</a>,
              <a href="https://github.com/WonsukYang">Wonsuk Yang</a>,
              <a href="https://yashbhalgat.github.io/">Yash Bhalgat</a>,
              <a href="https://www.hannahrosekirk.com/">Hannah Rose Kirk</a>,
              <strong> Aleksandar Shtedritski</strong>,
              <a href="https://maxbain.com/">Max Bain</a>
              <br>
              <em>AACL-IJNCLP</em>, 2022 &nbsp 
              <br>
              <a href="https://arxiv.org/abs/2203.11933">arXiv</a> /
              <!--<a href="data/prompt-array.bib">bibtex</a> /-->
              <a href="https://github.com/oxai/debias-vision-lang">code</a>
              <p>We propose a lightweight method to debias CLIP.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bias-gpt2.png" alt="b3do" width="150" height="150" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/1531beb762df4029513ebf9295e0d34f-Paper.pdf">
                <papertitle>Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models</papertitle>
              </a>
              <br>
              <a href="https://www.hannahrosekirk.com/">Hannah Rose Kirk</a>,
              <a href="https://yenniejun.com/">Yennie Jun</a>,
              Haider Iqbal,
              <a href="https://www.cs.ox.ac.uk/people/elias.benussi/">Elias Benussi</a>,
              <Filippo Volpin,
              <a href="https://scholar.google.com/citations?hl=en&user=YxO60fAAAAAJ&view_op=list_works&sortby=pubdate">Frederic A. Dreyer</a>,
              <strong> Aleksandar Shtedritski</strong>,
              <a href="https://yukimasano.github.io/">Yuki M. Aasano</a>
              <br>
              <em>NeurIPS</em>, 2021 &nbsp 
              <br>
              
              <a href="https://arxiv.org/abs/2102.04130">arXiv</a> /
              <!--<a href="data/bias-gpt2.bib">bibtex</a> /-->
              <a href="https://github.com/oxai/intersectional_gpt2">code</a>
              <p>An in-depth analysis of instersectional biases of GPT-2.</p>
            </td>
          </tr>


        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Services</heading>
              <p><strong>Reviewer</strong></p>
              <p>CVPR 2023, 2024, ICCV 2023, ECCV 2024 NeurIPS 2023, 2024</p>
              <p><strong>Teaching Assistant</strong></p>
              <ul style="text-align:justify;height: 150px;">
              <li> Information Engineering tutorial classes, Engineering Department, University of Oxford, 2021-22</li>
              <li> Image and Signal Processing lab, Engineering Department, University of Oxford, 2022</li>
              <li> Machine Learning tutorial classes, Department of Computer Science, University of Oxford, 2020-21</li>
              <li> Machine Learning revision classes, Magdalen College, University of Oxford, 2020-21</li>
              <li> Control Systems lab, Engineering Department, University of Oxford, 2021</li>
              </ul>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website template is borrowed from <a href="https://jonbarron.info/">Jon Barron</a>. Thanks!
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </tbody></table>

</body>

</html>
